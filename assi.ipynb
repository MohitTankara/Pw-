{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "#  Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "verfitting and underfitting are common issues in machine learning:\n\nOverfitting:\n\nDefinition: Occurs when a model learns the training data too well, including noise and outliers, to the point that it performs poorly on new, unseen data.\nConsequences: High training accuracy but low test accuracy, poor generalization, and high sensitivity to variations in the training data.\nMitigation:\nUse more training data.\nSimplify the model by reducing its complexity (e.g., fewer features or shallower neural networks).\nApply regularization techniques (e.g., L2 regularization).\nUse cross-validation to tune hyperparameters.\nEarly stopping during training.\nUnderfitting:\n\nDefinition: Occurs when a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and test data.\nConsequences: Low training and test accuracy, inability to capture complex relationships in the data.\nMitigation:\nIncrease model complexity (e.g., add more features, use deeper networks).\nGather more relevant features and data.\nChoose a more powerful algorithm.\nReduce underfitting by adjusting hyperparameters (e.g., increasing the number of training iterations).\nEnsemble methods like random forests or boosting to combine simple models.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# How can we reduce overfitting? Explain in brief.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "Increase training data.\nSimplify the model's complexity.\nApply regularization (L1, L2, dropout).\nUse cross-validation for hyperparameter tuning.\nEmploy early stopping.\nFeature selection and engineering.\nEnsemble methods.\nData augmentation.\nValidate the model's performance.\nMonitor and adjust hyperparameters.\n\n\n\n\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "Regularization in machine learning is a set of techniques used to prevent overfitting by adding a penalty term to the model's loss function. The penalty discourages the model from learning overly complex patterns in the training data, which can lead to poor generalization. The goal of regularization is to find a balance between fitting the training data well and avoiding excessive complexity in the model.\n\nHere are some common regularization techniques and how they work:\n\nL1 Regularization (Lasso):\n\nHow it works: Adds the absolute values of the model's coefficients as a penalty term to the loss function. It encourages the model to set some of the coefficients to exactly zero, effectively performing feature selection.\nUse case: Useful when you suspect that only a subset of features is relevant, and you want to automatically select the most important ones.\nL2 Regularization (Ridge):\n\nHow it works: Adds the squared values of the model's coefficients as a penalty term to the loss function. It discourages large coefficients and makes all features contribute somewhat to the prediction.\nUse case: Effective for reducing the impact of multicollinearity and preventing extreme parameter values.\nElastic Net Regularization:\n\nHow it works: Combines L1 and L2 regularization by adding both absolute and squared coefficients to the loss function. It offers a balance between feature selection (L1) and coefficient shrinkage (L2).\nUse case: Suitable when you want to both select relevant features and prevent large coefficient values.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Explain underfitting. List scenarios where underfitting can occur in ML. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the data, resulting in poor performance on both the training and test datasets. It represents a failure of the model to learn the data's complexities.\n\nScenarios where underfitting can occur in machine learning:\nInsufficient Data: When the training dataset is too small, the model may not have enough information to learn the true relationships in the data, leading to underfitting.\n\nIgnoring Important Patterns: Disregarding essential information or patterns in the data can lead to underfitting. For instance, ignoring a time-related trend in time series data.\n\nOver-regularization: Excessive use of regularization techniques, such as setting very high regularization penalties, can constrain the model too much, causing underfitting.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#  Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "Detecting overfitting and underfitting in machine learning models is crucial for building models that generalize well to unseen data. Here are some common methods for detecting these issues and how to determine whether your model is overfitting or underfitting:\n    Validation and Test Performance:\n\nEvaluate your model on a separate validation set and a test set.\nOverfitting: If the model performs well on the training data but poorly on the validation and test data, it's a sign of overfitting.\nUnderfitting: If the model performs poorly on all datasets, it might be underfitting.\n\nBias-Variance Trade-off:\n\nUnderstand the bias-variance trade-off. High bias typically leads to underfitting, while high variance leads to overfitting.\nAdjust model complexity and other factors to strike a balance.\n\n\nDomain Knowledge:\n\nLeverage domain knowledge to assess the reasonableness of model predictions.\nOverfitting: Unreasonable predictions or extreme values might indicate overfitting.\nUnderfitting: Consistently poor predictions are a sign of underfitting.\n\n\n\nDetermining whether your model is overfitting or underfitting often involves a combination of these methods. It's essential to strike the right balance between model complexity and generalization to build robust machine learning models.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}